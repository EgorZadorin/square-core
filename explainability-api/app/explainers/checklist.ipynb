{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on how to run Checklist (aka Behavioral Tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add functionality to process json to be injected into the db\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import itertools\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist is a list of predefined questions that evaluate the behavior of a model (SQuARE Skill). In particular, it evalutes whether the Skill has certain abilities such as understanding comparisons, understanding coreference, or even it let us know if the model has gender biases (eg: he is a doctor, she is a nurse).\n",
    "\n",
    "\n",
    "To do this we need the following 2 methods: `create_query` and `predict`. For simplicity I copied them below but you can find them on checklist.py in your current folder.\n",
    "\n",
    "We need first the Skill we want to evaluate and also the list of test_cases. You can find all test cases in the folder explainability-api/checklists. We have test cases for extractive skills, for multiple-choice and for boolean skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query(skill, test_cases: List):\n",
    "    \"\"\"\n",
    "    Creates a query and make it suitable for sending to for prediction\n",
    "\n",
    "    Args:\n",
    "        skill: input skill for which the checklist tests are run\n",
    "        test_cases (list) : Test cases as a list\n",
    "\n",
    "    Returns:\n",
    "        json_object (json object) : A json object containing the test case and its prediction\n",
    "        answer (str) : Prediction for test case made by the skill\n",
    "\n",
    "    \"\"\"\n",
    "    skill_type = skill[\"skill_type\"]\n",
    "    base_model = skill[\"default_skill_args\"].get(\"base_model\")\n",
    "    adapter = skill[\"default_skill_args\"].get(\"adapter\")\n",
    "    # extract all tests\n",
    "    all_tests = [tests[\"test_cases\"] for tests in test_cases]\n",
    "    # all_tests = list(itertools.chain.from_iterable([tests[\"test_cases\"] for tests in test_cases]))\n",
    "    questions, contexts, answers = list(), list(), list()\n",
    "\n",
    "    test_type = list(itertools.chain.from_iterable([[test[\"test_type\"]] * len(test[\"test_cases\"])\n",
    "                                                    for test in test_cases]))\n",
    "    capability = list(itertools.chain.from_iterable([[test[\"capability\"]] * len(test[\"test_cases\"])\n",
    "                                                    for test in test_cases]))\n",
    "    test_name = list(itertools.chain.from_iterable([[test[\"test_name\"]] * len(test[\"test_cases\"])\n",
    "                                                    for test in test_cases]))\n",
    "\n",
    "    for tests in all_tests:\n",
    "        questions.append([query[\"question\"] for query in tests])\n",
    "        # list of list for mcq else list\n",
    "        contexts.append([query[\"context\"] if skill_type != \"multiple-choice\"\n",
    "                         else query[\"context\"] + \"\\n\" + \"\\n\".join(query[\"options\"])\n",
    "                         for query in tests])\n",
    "        answers.extend([query.get(\"answer\") if \"answer\" in query.keys() else query.get(\"prediction_before_change\")\n",
    "                        for query in tests])\n",
    "\n",
    "        # TODO\n",
    "        # send batch to the skill query endpoint\n",
    "\n",
    "    prediction_requests = list()\n",
    "    # create the prediction request\n",
    "    for idx in range(len(questions)):\n",
    "        for question, context in zip(questions[idx], contexts[idx]):\n",
    "            request = dict()\n",
    "            request[\"num_results\"] = 1\n",
    "            request[\"user_id\"] = \"ukp\"\n",
    "            request[\"skill_args\"] = {\"base_model\": base_model, \"adapter\": adapter, \"context\": context}\n",
    "            request[\"query\"] = question\n",
    "            prediction_requests.append(request)\n",
    "\n",
    "    model_inputs = dict()\n",
    "    model_inputs[\"request\"] = prediction_requests\n",
    "    model_inputs[\"answers\"] = answers\n",
    "    model_inputs[\"test_type\"] = test_type\n",
    "    model_inputs[\"capability\"] = capability\n",
    "    model_inputs[\"test_name\"] = test_name\n",
    "    # logger.info(\"inputs:\", model_inputs)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def predict(model_inputs: dict, skill_id: str) -> list:\n",
    "    \"\"\"\n",
    "    Predicts a given query\n",
    "\n",
    "    Args:\n",
    "        model_inputs (dict) : input for the model inference\n",
    "        skill_id (str) : id of skill for which the predictions need to be run\n",
    "\n",
    "    Returns:\n",
    "        Returns the model predictions and success rate\n",
    "    \"\"\"\n",
    "    model_outputs = list()\n",
    "    try:\n",
    "        headers = {'Content-type': 'application/json'}\n",
    "        skill_query_url = f\"https://square.ukp-lab.de/api/skill-manager/skill/{skill_id}/query\" #note I hardcoded square URL here\n",
    "        model_predictions = list()\n",
    "        # i = 0\n",
    "        for request in model_inputs[\"request\"]:\n",
    "            response = requests.post(skill_query_url, data=json.dumps(request), headers=headers)\n",
    "            predictions = response.json()\n",
    "            model_predictions.append(predictions[\"predictions\"][0][\"prediction_output\"][\"output\"])\n",
    "            # i += 1\n",
    "            # if i == 10:\n",
    "            #     break\n",
    "\n",
    "        # calculate success rate\n",
    "        success_rate = [pred == gold for pred, gold in zip(model_predictions, model_inputs[\"answers\"])]\n",
    "\n",
    "        for test_type, capability, test_name, request, answer, prediction, success in zip(\n",
    "            model_inputs[\"test_type\"],\n",
    "            model_inputs[\"capability\"],\n",
    "            model_inputs[\"test_name\"],\n",
    "            model_inputs[\"request\"],\n",
    "            model_inputs[\"answers\"],\n",
    "            model_predictions,\n",
    "            success_rate\n",
    "        ):\n",
    "            model_outputs.append(\n",
    "                {\n",
    "                    \"skill_id\": skill_id,\n",
    "                    \"test_type\": test_type,\n",
    "                    \"capability\": capability,\n",
    "                    \"test_name\": test_name,\n",
    "                    \"question\": request[\"query\"],\n",
    "                    \"context\": request[\"skill_args\"][\"context\"],\n",
    "                    \"answer\": answer,\n",
    "                    \"prediction\": prediction,\n",
    "                    \"success\": success\n",
    "                }\n",
    "            )\n",
    "        # print(model_outputs)\n",
    "    except Exception as ex:\n",
    "        logger.info(ex)\n",
    "    return model_outputs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get SQuARE's Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_response = requests.get(\"https://square.ukp-lab.de/api/skill-manager/skill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = skills_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '63a6246c2e30fd4c06f7a0be',\n",
       " 'name': 'SpanBert - NaturalQuestionsShort',\n",
       " 'url': 'http://extractive-qa',\n",
       " 'skill_type': 'span-extraction',\n",
       " 'skill_settings': {'requires_context': True, 'requires_multiple_choices': 0},\n",
       " 'user_id': 'puerto',\n",
       " 'created_at': '2022-12-23T21:58:04.046000',\n",
       " 'skill_input_examples': [{'query': '', 'context': '', 'choices': None},\n",
       "  {'query': '', 'context': '', 'choices': None},\n",
       "  {'query': '', 'context': '', 'choices': None}],\n",
       " 'description': '',\n",
       " 'default_skill_args': {'base_model': 'haritzpuerto/spanbert-large-cased_NaturalQuestionsShort'},\n",
       " 'published': True,\n",
       " 'client_id': 'puerto-SpanBert - NaturalQuestionsShort',\n",
       " 'client_secret': None,\n",
       " 'data_sets': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../checklists/extractive_model_tests.json\") as f:\n",
    "    extractive_model_tests = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's make the query\n",
    "\n",
    "The query is actually a list of queries to the Skill. For each test, we gotta make a query to the Skill to get the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = create_query(skills[-1], extractive_model_tests['tests'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['request', 'answers', 'test_type', 'capability', 'test_name'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of getting some result quickly, I will just use 1 test, but you should try with all tests too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_min = query.copy()\n",
    "query_min['request'] = query_min['request'][:1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get the predictions of the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'skill_id': '63a6246c2e30fd4c06f7a0be',\n",
       "  'test_type': 'MFT',\n",
       "  'capability': 'Vocabulary',\n",
       "  'test_name': 'A is COMP than B. Who is more / less COMP??',\n",
       "  'question': 'Who is less nice?',\n",
       "  'context': 'Caroline is nicer than Marie.',\n",
       "  'answer': 'Marie',\n",
       "  'prediction': 'Caroline',\n",
       "  'success': False}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(query_min, skills[-1]['id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the `predicition` is different from the `answer` (label). This shows that the model is not understanding comparisons properly. If this happens in most comparisons tests, this will indicate that the model is weak when doing comparisons, and shouldn't be used for that case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should run at least one extractive QA Skill with the tests (checklist). I would recommend the one of the example above or any with the name `SpanBert`-{DATASET} because these Skills should be high performing, so it would be interesting to know if they are always robust or if there is any kind of reasoning where it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f4b1df9402f5c9ae5cac5f40ef98bbd391cfcb59c4c0c638fee2fda7003ede2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
